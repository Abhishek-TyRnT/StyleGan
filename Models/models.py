# -*- coding: utf-8 -*-
"""Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OK630ZZBTbWI-r7s6K4kztN0_up_jiaJ
"""

import tensorflow as tf
import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.initializers import RandomNormal
from tensorflow.keras.layers import Layer
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.initializers import Ones
from tensorflow.keras.layers import Reshape
from tensorflow.keras.layers import UpSampling2D
from tensorflow.keras.layers import AvgPool2D
from tensorflow.keras.models import clone_model
from tensorflow.keras.layers import Flatten

#Mapping network for generating latent code
#It is an 8 layer Maaping Network
def get_mapping_network(inp):
  x = Dense(512,kernel_initializer = RandomNormal(0,1.0))(inp)
  x = LeakyReLU(0.2)(x)
  x = Dense(512,kernel_initializer = RandomNormal(0,1.0))(x)
  x = LeakyReLU(0.2)(x)
  x = Dense(512,kernel_initializer = RandomNormal(0,1.0))(x)
  x = LeakyReLU(0.2)(x)
  x = Dense(512,kernel_initializer = RandomNormal(0,1.0))(x)
  x = LeakyReLU(0.2)(x)
  x = Dense(512,kernel_initializer = RandomNormal(0,1.0))(x)
  x = LeakyReLU(0.2)(x)
  x = Dense(512,kernel_initializer = RandomNormal(0,1.0))(x)
  x = LeakyReLU(0.2)(x)
  x = Dense(512,kernel_initializer = RandomNormal(0,1.0))(x)
  x = LeakyReLU(0.2)(x)
  x = Dense(512,kernel_initializer = RandomNormal(0,1.0))(x)
  x = LeakyReLU(0.2,name = 'MappingNetworkOutput')(x)
  MAP_Network = Model(inputs = inp,outputs = x)
  return MAP_Network


class Mapping_Network(Model):
  def __init__(self,block_no = 1,names = 'MappingNetwork'):
    super(Mapping_Network,self).__init__(name = names)
    inp                  = Input(shape = (512,),name = 'LatentInput')
    self.block_no = block_no 
    self.mapping_network = get_mapping_network(inp)
  
  def add_block(self):
    self.block_no += 1
  def call(self,inp1,inp2):
    output_1     = self.mapping_network(inp1)
    output_2     = self.mapping_network(inp2)
    output_index = np.random.choice(range(2*self.block_no),size = (self.block_no),replace = False)
    output       = {}
    k            = 0
    for i in range(self.block_no):
      for j in range(2):
        input_name = 'LatentInput' + str(j+1) + str(i+1)
        if k in output_index:
          output.update({input_name:output_1})
        else:
          output.update({input_name:output_2})
        k += 1
    
    return output

class AffineTransform(Layer):
  def __init__(self,fmaps,names = ''):
    super(AffineTransform,self).__init__(name = names)
    self.fmaps = fmaps
    self.dense = Dense(fmaps*2,kernel_initializer=RandomNormal(0,1.0),bias_initializer=Ones())
    self.activation = LeakyReLU(alpha = 0.2)
    self.reshape = Reshape((self.fmaps,2))
  def call(self,inp):
    x = inp
    x = self.dense(x)
    x = self.activation(x)
    x = self.reshape(x)
    return x

class AdaIn(Layer):
  def __init__(self,fmaps,names):
    super(AdaIn,self).__init__(name = names)
    self.fmaps = fmaps
  
  def call(self,scaling_parameters,filter_maps):
    result = []
    batch_size = filter_maps.shape[0]
    normalized =  self.normalize(filter_maps)
    y_s,y_b = scaling_parameters[:,:,0],scaling_parameters[:,:,1]
    y_s = tf.expand_dims(tf.expand_dims(y_s,1),1)
    y_b = tf.expand_dims(tf.expand_dims(y_b,1),1)

    result = y_s*normalized + y_b
    return result   
  def normalize(self,fmaps,epsilon = 1e-8):
      mean = tf.reduce_mean(fmaps,[1,2],keepdims=True)
      std  = tf.sqrt(tf.reduce_mean((fmaps - mean)**2 + epsilon,[1,2],keepdims=True))

      normalized = (fmaps- mean)/std

      return normalized

class Synthesis_block(Layer):
  def __init__(self,multiplier = 2,filter_base = 8192,names = ""):
    super(Synthesis_block,self).__init__(name = names)
    filters= min(int(8192/2**(multiplier + 2)),512)
    self.conv_1 = Conv2D(filters,kernel_size=(3,3),padding = 'same',kernel_initializer=RandomNormal(0,1))
    self.conv_2 = Conv2D(filters,kernel_size=(3,3),padding = 'same',kernel_initializer=RandomNormal(0,1))
    self.upsample = UpSampling2D(size = (2,2),interpolation='bilinear')
    self.activation_1 = LeakyReLU(0.2)
    self.activation_2 = LeakyReLU(0.2)
    self.AdaIn_1 = AdaIn(filters,names  + 'AdaIn')
    self.AdaIn_2 = AdaIn(filters,names  + 'AdaIn')
  def call(self,inp,noise_1,noise_2,transform_1,transform_2):
    x = self.upsample(inp)
    x = self.conv_1(x)
    x = self.activation_1(x)
    x = x + noise_1
    x = self.AdaIn_1(transform_1,x)
    x = self.conv_2(x)
    x = self.activation_2(x)
    x = x + noise_2
    x = self.AdaIn_2(transform_2,x)
    return x

def first_block():
  const_input     = Input(shape =   (None,None,512),name = 'ConstantInput')
  noise_input_1   = Input(shape = (None,None,512),name = 'Noise11')
  noise_input_2   = Input(shape = (None,None,512),name = 'Noise21')
  latent_input_1  = Input(shape = (512,),name = 'LatentInput11')
  latent_input_2  = Input(shape = (512,),name = 'LatentInput21')
  network         = const_input + noise_input_1
  transformed     = AffineTransform(512)(latent_input_1)
  network         = AdaIn(512,names = 'AdaIN11')(transformed,network)
  network         = Conv2D(512,kernel_size = (3,3),padding = 'SAME',kernel_initializer=RandomNormal(0,1))(network)
  transformed     = AffineTransform(512)(latent_input_2)
  network         = AdaIn(512,names = 'AdaIN12')(transformed,network)
  network         = Conv2D(3,kernel_size = (3,3),padding = 'same',kernel_initializer=RandomNormal(0,1),activation = 'relu')(network)
  model           = Model(inputs = [const_input,noise_input_1,noise_input_2,latent_input_1,latent_input_2],outputs = network)
  return model

def add_new_block(old_model,block_no):
  inputs = old_model.inputs
  def no_of_filters(stage):
     return min(int(8192/(2**(stage+2))),512)
  latent_input_1 = Input(shape = (512,),name = 'LatentInput1'+str(block_no))
  latent_input_2 = Input(shape = (512,),name = 'LatentInput2'+str(block_no))
  noise_input_1  = Input(shape = (None,None,None),name = 'Noise1'+str(block_no))
  noise_input_2  = Input(shape = (None,None,None),name = 'Noise2'+str(block_no))
  transformed1   = AffineTransform(no_of_filters(block_no))(latent_input_1)
  transformed2   = AffineTransform(no_of_filters(block_no))(latent_input_2)
  network        = old_model.layers[-1].input
  network        = Synthesis_block(block_no)(network,noise_input_1,noise_input_2,transformed1,transformed2)
  network        = Conv2D(filters = 3,kernel_size=(1,1),padding = 'SAME',name = 'to_rgb_'+str(block_no),activation='relu')(network)
  new_model      = Model(inputs = inputs + [noise_input_1,noise_input_2,latent_input_1,latent_input_2],outputs = network)
  return new_model

class Generator(Model):
  def __init__(self,names = 'Generator'):
    super(Generator,self).__init__(name = names)
    self.model = first_block()
    self.mapping_network = Mapping_Network()
    self.block_no = 1
  
  def call(self,inps,**kwargs):
    model_inputs = self.model.inputs
    inputs = []
    latent_inputs = inps['LatentInput']
    latent_inputs = self.mapping_network(latent_inputs[0],latent_inputs[1])
    inps.update(latent_inputs)
    inputs = {}
    for input in model_inputs:
      name = input.name
      print(name,end = "  ")
      try:
        name = name[:(name.index('_'))]
      except:
        name = name[:(name.index(':'))]
      print(name)
      inputs[name] = inps[name]
    return self.model(inputs)
    
  def increase_resolution(self):
    new_model = add_new_block(self.model,block_no = self.block_no+1)
    self.block_no+=1
    self.mapping_network.add_block()
    self.model = new_model

class Discriminator(Model):
  def __init__(self,names = 'Discriminator'):
    super(Discriminator,self).__init__(name = names)
    self.block_no = 1
    self.discriminator = self.first_block()
  def first_block(self):
    inp = Input(shape = (4,4,3))
    network = Conv2D(512,(3,3),1,padding='SAME',kernel_initializer=RandomNormal(0,1))(inp)
    network = LeakyReLU(0.2)(network)
    network = Conv2D(512,(3,3),1,padding='SAME',kernel_initializer=RandomNormal(0,1))(network)
    network = LeakyReLU(0.2)(network)
    network = AvgPool2D(pool_size=(4,4))(network)
    network = Flatten()(network)
    network = Dense(512,kernel_initializer= RandomNormal(0,1))(network)
    network = LeakyReLU(0.2)(network)
    network = Dense(1,kernel_initializer= RandomNormal(0,1))(network) 
    model = Model(inputs = inp,outputs = network)
    return model
  
  def add_block(self):
    def no_of_filters(stage):
     return min(int(8192/(2**(stage+2))),512)
    inp = Input(shape = (None,None,3))
    
    network = Conv2D(no_of_filters(self.block_no),(3,3),1,padding='SAME',kernel_initializer=RandomNormal(0,1))(inp)
    network = LeakyReLU(0.2)(network)
    network = Conv2D(no_of_filters(self.block_no -1),(3,3),1,padding='SAME',kernel_initializer=RandomNormal(0,1))(network)
    network = LeakyReLU(0.2)(network)
    network = AvgPool2D()(network)
    for layer in self.discriminator.layers[3:]:
      network = layer(network)
    self.block_no+=1
    model = Model(inputs = inp,outputs = network)
    
    self.discriminator = model

  def call(self,inp):
    output = self.discriminator(inp)
    return output
    
